{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "from itertools import accumulate\n",
    "from functools import reduce\n",
    "\n",
    "from CLR_preview import CyclicLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_urls = {\n",
    "    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n",
    "    'densenet121': 'https://download.pytorch.org/models/densenet121-241335ed.pth',\n",
    "    'densenet169': 'https://download.pytorch.org/models/densenet169-6f0f7f60.pth',\n",
    "    'densenet201': 'https://download.pytorch.org/models/densenet201-4c113574.pth',\n",
    "    'densenet161': 'https://download.pytorch.org/models/densenet161-17b70270.pth',\n",
    "    #truncated _google to match module name\n",
    "    'inception_v3': 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth',\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',    \n",
    "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
    "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
    "    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n",
    "    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n",
    "    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n",
    "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',    \n",
    "}\n",
    "\n",
    "model_names = model_urls.keys()\n",
    "\n",
    "input_sizes = {\n",
    "    'alexnet' : (224,224),\n",
    "    'densenet': (224,224),\n",
    "    'resnet' : (224,224),\n",
    "    'inception' : (299,299),\n",
    "    'squeezenet' : (224,224),#not 255,255 acc. to https://github.com/pytorch/pytorch/issues/1120\n",
    "    'vgg' : (224,224)\n",
    "}\n",
    "\n",
    "models_to_test = ['alexnet', 'inception_v3', 'densenet169',\n",
    "                  'resnet34', 'squeezenet1_1', 'vgg13',]\n",
    "\n",
    "data_dir = 'hymenoptera_data'\n",
    "\n",
    "train_subfolder = os.path.join(data_dir, 'train')\n",
    "classes = [d.split(train_subfolder, 1)[1] for d in \\\n",
    "           glob(os.path.join(train_subfolder, '**'))]\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "epoch_multiplier = 8 #per class and times 1(shallow), 2(deep), 4(from_scratch)\n",
    "use_gpu = torch.cuda.is_available()\n",
    "use_clr = False\n",
    "\n",
    "#Assume 50 examples per class and CLR authors' middle ground\n",
    "clr_stepsize = (len(classes)*50//batch_size)*4\n",
    "\n",
    "\n",
    "print(\"Shootout of model(s) %s with batch_size %d running on CUDA %s \" % \\\n",
    "      (\", \".join(models_to_test), batch_size, use_gpu) + \\\n",
    "      \"with CLR %s for %d classes on data %s.\" % (use_clr, len(classes), data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic pretrained model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We solve the dimensionality mismatch between\n",
    "#final layers in the constructed vs pretrained\n",
    "#modules at the data level.\n",
    "def diff_states(dict_canonical, dict_subset):\n",
    "    names1, names2 = (list(dict_canonical.keys()), list(dict_subset.keys()))\n",
    "    \n",
    "    #Sanity check that param names overlap\n",
    "    #Note that params are not necessarily in the same order\n",
    "    #for every pretrained model\n",
    "    not_in_1 = [n for n in names1 if n not in names2]\n",
    "    not_in_2 = [n for n in names2 if n not in names1]\n",
    "    assert len(not_in_1) == 0\n",
    "    assert len(not_in_2) == 0\n",
    "\n",
    "    for name, v1 in dict_canonical.items():\n",
    "        v2 = dict_subset[name]\n",
    "        assert hasattr(v2, 'size')\n",
    "        if v1.size() != v2.size():\n",
    "            yield (name, v1)                \n",
    "\n",
    "def load_model_merged(name, num_classes):\n",
    "    \n",
    "    model = models.__dict__[name](num_classes=num_classes)\n",
    "    \n",
    "    #Densenets don't (yet) pass on num_classes, hack it in for 169\n",
    "    if name == 'densenet169':\n",
    "        model = torchvision.models.DenseNet(num_init_features=64, growth_rate=32, \\\n",
    "                                            block_config=(6, 12, 32, 32), num_classes=num_classes)\n",
    "        \n",
    "    pretrained_state = model_zoo.load_url(model_urls[name])\n",
    "\n",
    "    #Diff\n",
    "    diff = [s for s in diff_states(model.state_dict(), pretrained_state)]\n",
    "    print(\"Replacing the following state from initialized\", name, \":\", \\\n",
    "          [d[0] for d in diff])\n",
    "    \n",
    "    for name, value in diff:\n",
    "        pretrained_state[name] = value\n",
    "    \n",
    "    assert len([s for s in diff_states(model.state_dict(), pretrained_state)]) == 0\n",
    "    \n",
    "    #Merge\n",
    "    model.load_state_dict(pretrained_state)\n",
    "    return model, diff\n",
    "\n",
    "\n",
    "def filtered_params(net, param_list=None):\n",
    "    def in_param_list(s):\n",
    "        for p in param_list:\n",
    "            if s.endswith(p):\n",
    "                return True\n",
    "        return False    \n",
    "    #Caution: DataParallel prefixes '.module' to every parameter name\n",
    "    params = net.named_parameters() if param_list is None \\\n",
    "    else (p for p in net.named_parameters() if \\\n",
    "          in_param_list(p[0]) and p[1].requires_grad)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Todo: split function into separate test and train data\n",
    "#To get the tutorial data (bee vs. ants), go to:\n",
    "#http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "def get_data(resize):\n",
    "\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomSizedCrop(max(resize)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            #Higher scale-up for inception\n",
    "            transforms.Scale(int(max(resize)/224*256)),\n",
    "            transforms.CenterCrop(max(resize)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    dsets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "             for x in ['train', 'val']}\n",
    "    dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=batch_size,\n",
    "                                                   shuffle=True)\n",
    "                    for x in ['train', 'val']}\n",
    "    dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "    dset_classes = dsets['train'].classes\n",
    "    \n",
    "    return dset_loaders['train'], dset_loaders['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, epochs, param_list=None, CLR=False):\n",
    "    #Todo: DRY\n",
    "    def in_param_list(s):\n",
    "        for p in param_list:\n",
    "            if s.endswith(p):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if use_gpu:\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    #If finetuning model, turn off grad for other params and make sure to turn on others\n",
    "    for p in net.named_parameters():\n",
    "        p[1].requires_grad = (param_list is None) or in_param_list(p[0])\n",
    "\n",
    "    params = (p for p in filtered_params(net, param_list))\n",
    "\n",
    "    #Optimizer as in tutorial\n",
    "    optimizer = optim.SGD((p[1] for p in params), lr=0.001, momentum=0.9)\n",
    "    if CLR:\n",
    "            \n",
    "        global clr_stepsize\n",
    "        clr_wrapper = CyclicLR(optimizer, step_size=clr_stepsize)\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            if use_gpu:\n",
    "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda(async=True))\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            loss = None\n",
    "            # for nets that have multiple outputs such as inception\n",
    "            if isinstance(outputs, tuple):\n",
    "                loss = sum((criterion(o,labels) for o in outputs))\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if CLR:\n",
    "                clr_wrapper.batch_step()\n",
    "            \n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            if i % 30 == 29:\n",
    "                avg_loss = running_loss / 30\n",
    "                losses.append(avg_loss)\n",
    "                \n",
    "                lrs = [p['lr'] for p in optimizer.param_groups]\n",
    "                    \n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, avg_loss), lrs)\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    return losses\n",
    "\n",
    "#Get stats for training and evaluation in a structured way\n",
    "#If param_list is None all relevant parameters are tuned,\n",
    "#otherwise, only parameters that have been constructed for custom\n",
    "#num_classes\n",
    "def train_stats(m, trainloader, epochs, param_list=None, CLR=False):\n",
    "    stats = {}\n",
    "    params = filtered_params(m, param_list)    \n",
    "    counts = 0,0\n",
    "    for counts in enumerate(accumulate((reduce(lambda d1,d2: d1*d2, p[1].size()) for p in params)) ):\n",
    "        pass\n",
    "    stats['variables_optimized'] = counts[0] + 1\n",
    "    stats['params_optimized'] = counts[1]\n",
    "    \n",
    "    before = time.time()\n",
    "    losses = train(m, trainloader, epochs, param_list=param_list, CLR=CLR)\n",
    "    stats['training_time'] = time.time() - before\n",
    "\n",
    "    stats['training_loss'] = losses[-1] if len(losses) else float('nan')\n",
    "    stats['training_losses'] = losses\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def evaluate_stats(net, testloader):\n",
    "    stats = {}\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    before = time.time()\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        images, labels = data\n",
    "\n",
    "        if use_gpu:\n",
    "            images, labels = (images.cuda()), (labels.cuda(async=True))\n",
    "\n",
    "        outputs = net(Variable(images))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    accuracy = correct / total\n",
    "    stats['accuracy'] = accuracy\n",
    "    stats['eval_time'] = time.time() - before\n",
    "    \n",
    "    print('Accuracy on test images: %f' % accuracy)\n",
    "    return stats\n",
    "\n",
    "\n",
    "def train_eval(net, trainloader, testloader, epochs, param_list=None, CLR=False):\n",
    "    print(\"Training...\" if not param_list else \"Retraining...\")\n",
    "    stats_train = train_stats(net, trainloader, epochs, param_list=param_list, CLR=CLR)\n",
    "    \n",
    "    print(\"Evaluating...\")\n",
    "    net = net.eval()\n",
    "    stats_eval = evaluate_stats(net, testloader)\n",
    "    \n",
    "    return {**stats_train, **stats_eval}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    stats = []\n",
    "    num_classes = len(classes)\n",
    "    \n",
    "    epochs = num_classes * epoch_multiplier * 1\n",
    "    print(\"RETRAINING %d epochs\" % epochs)\n",
    "\n",
    "    for name in models_to_test:\n",
    "        print(\"\")\n",
    "        print(\"Targeting %s with %d classes\" % (name, num_classes))\n",
    "        print(\"------------------------------------------\")\n",
    "        model_pretrained, diff = load_model_merged(name, num_classes)\n",
    "        final_params = [d[0] for d in diff]\n",
    "\n",
    "        resize = [s[1] for s in input_sizes.items() if s[0] in name][0]\n",
    "        print(\"Resizing input images to max of\", resize)\n",
    "        trainloader, testloader = get_data(resize)\n",
    "\n",
    "        if use_gpu:\n",
    "            print(\"Transfering models to GPU(s)\")\n",
    "            model_pretrained = torch.nn.DataParallel(model_pretrained).cuda()\n",
    "\n",
    "        pretrained_stats = train_eval(model_pretrained, \n",
    "                                      trainloader, testloader, epochs,\n",
    "                                      final_params, use_clr)\n",
    "        pretrained_stats['name'] = name\n",
    "        pretrained_stats['retrained'] = True\n",
    "        pretrained_stats['shallow_retrain'] = True\n",
    "        stats.append(pretrained_stats)\n",
    "\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    print(\"---------------------\")\n",
    "    \n",
    "    epochs = num_classes * epoch_multiplier * 4\n",
    "    print(\"TRAINING %d epochs from scratch\" % epochs)\n",
    "    \n",
    "    for name in models_to_test:\n",
    "        print(\"\")    \n",
    "        print(\"Targeting %s with %d classes\" % (name, num_classes))\n",
    "        print(\"------------------------------------------\")\n",
    "        model_blank = models.__dict__[name](num_classes=num_classes)\n",
    "\n",
    "        resize = [s[1] for s in input_sizes.items() if s[0] in name][0]\n",
    "        print(\"Resizing input images to max of\", resize)\n",
    "        trainloader, testloader = get_data(resize)\n",
    "\n",
    "        if use_gpu:\n",
    "            print(\"Transfering models to GPU(s)\")\n",
    "            model_blank = torch.nn.DataParallel(model_blank).cuda()    \n",
    "\n",
    "        blank_stats = train_eval(model_pretrained, trainloader, testloader, epochs,\n",
    "                                 CLR=use_clr)\n",
    "        blank_stats['name'] = name\n",
    "        blank_stats['retrained'] = False\n",
    "        blank_stats['shallow_retrain'] = False\n",
    "        stats.append(blank_stats)\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "    t = 0.0\n",
    "    for s in stats:\n",
    "        t += s['eval_time'] + s['training_time']\n",
    "    print(\"Total time for training and evaluation\", t)\n",
    "    print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    epochs = num_classes * epoch_multiplier * 2\n",
    "    print(\"RETRAINING %d epochs deeply\" % epochs)\n",
    "\n",
    "    for name in models_to_test:\n",
    "        print(\"\")\n",
    "        print(\"Targeting %s with %d classes\" % (name, num_classes))\n",
    "        print(\"------------------------------------------\")\n",
    "        model_pretrained, diff = load_model_merged(name, num_classes)\n",
    "\n",
    "        resize = [s[1] for s in input_sizes.items() if s[0] in name][0]\n",
    "        print(\"Resizing input images to max of\", resize)\n",
    "        trainloader, testloader = get_data(resize)\n",
    "\n",
    "        if use_gpu:\n",
    "            print(\"Transfering models to GPU(s)\")\n",
    "            model_pretrained = torch.nn.DataParallel(model_pretrained).cuda()\n",
    "\n",
    "        pretrained_stats = train_eval(model_pretrained, trainloader, testloader, None,\n",
    "                                     CLR=use_clr)\n",
    "        pretrained_stats['name'] = name\n",
    "        pretrained_stats['retrained'] = True\n",
    "        pretrained_stats['shallow_retrain'] = False\n",
    "        stats.append(pretrained_stats)\n",
    "\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export stats as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    import csv\n",
    "    with open(dat_dir+('_clr' if use_clr else '')+'.csv', 'w') as csvfile:\n",
    "        fieldnames = stats[0].keys()\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for s in stats:\n",
    "            writer.writerow(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
